{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "114b1169",
   "metadata": {},
   "source": [
    "# Sample usage for tokenize\n",
    "# Regression Tests: NLTKWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9fd20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c00e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"On a $50,000 mortgage of 30 years at 8 percent, the monthly payment would be $366.88.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b195462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05cf7afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f2ee5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On',\n",
       " 'a',\n",
       " '$',\n",
       " '50,000',\n",
       " 'mortgage',\n",
       " 'of',\n",
       " '30',\n",
       " 'years',\n",
       " 'at',\n",
       " '8',\n",
       " 'percent',\n",
       " ',',\n",
       " 'the',\n",
       " 'monthly',\n",
       " 'payment',\n",
       " 'would',\n",
       " 'be',\n",
       " '$',\n",
       " '366.88',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_tokens = word_tokenize(s1)\n",
    "s1_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9d4f081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s1_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97efc6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = \"\\\"We beat some pretty good teams to get here,\\\" Slocum said.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d71e05ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b920ad40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['``',\n",
       " 'We',\n",
       " 'beat',\n",
       " 'some',\n",
       " 'pretty',\n",
       " 'good',\n",
       " 'teams',\n",
       " 'to',\n",
       " 'get',\n",
       " 'here',\n",
       " ',',\n",
       " \"''\",\n",
       " 'Slocum',\n",
       " 'said',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2_tokens = word_tokenize(s2)\n",
    "s2_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "366ed150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2af7fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = \"Well, we couldn't have this predictable, cliche-ridden, \\\"Touched by an Angel\\\" (a show creator John Masius worked on) wanna-be if she didn't.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "578f1fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c5fb557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Well',\n",
       " ',',\n",
       " 'we',\n",
       " 'could',\n",
       " \"n't\",\n",
       " 'have',\n",
       " 'this',\n",
       " 'predictable',\n",
       " ',',\n",
       " 'cliche-ridden',\n",
       " ',',\n",
       " '``',\n",
       " 'Touched',\n",
       " 'by',\n",
       " 'an',\n",
       " 'Angel',\n",
       " \"''\",\n",
       " '(',\n",
       " 'a',\n",
       " 'show',\n",
       " 'creator',\n",
       " 'John',\n",
       " 'Masius',\n",
       " 'worked',\n",
       " 'on',\n",
       " ')',\n",
       " 'wanna-be',\n",
       " 'if',\n",
       " 'she',\n",
       " 'did',\n",
       " \"n't\",\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_tokens = word_tokenize(s3)\n",
    "s3_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "989afb49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s3_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f845976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s4 = \"I cannot cannot work under these conditions!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b78aa9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31c37a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'can', 'not', 'can', 'not', 'work', 'under', 'these', 'conditions', '!']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s4_tokens = word_tokenize(s4)\n",
    "s4_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50734fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s4_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9ffd6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s5 = \"The company spent $30,000,000 last year.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffa7ed3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'company', 'spent', '$', '30,000,000', 'last', 'year', '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s5_tokens = word_tokenize(s5)\n",
    "s5_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4809c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "747a319a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'company',\n",
       " 'spent',\n",
       " '40.75',\n",
       " '%',\n",
       " 'of',\n",
       " 'its',\n",
       " 'income',\n",
       " 'last',\n",
       " 'year',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s6 = \"The company spent 40.75% of its income last year.\"\n",
    "s6_tokens = word_tokenize(s6)\n",
    "s6_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4ebe073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ae95ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s8 = \"I bought these items: books, pencils, and pens.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3acc998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I bought these items: books, pencils, and pens.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6985c85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I bought these items: books, pencils, and pens.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s8_sent = sent_tokenize(s8)\n",
    "s8_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a39aa52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s8_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56c0b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "s9 = \"Though there were 150, 100 of them were old.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1d6ec6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Though there were 150, 100 of them were old.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s9_sent = sent_tokenize(s9)\n",
    "s9_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bab0e6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Though there were 150, 100 of them were old.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s9_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7a2577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s10 = \"There were 300,000, but that wasn't enough.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ced241f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"There were 300,000, but that wasn't enough.\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s10_sent = sent_tokenize(s10)\n",
    "s10_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3379e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57f4cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s11 = \"It's more'n enough.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3359635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It's more'n enough.\"]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s11_sent = sent_tokenize(s11)\n",
    "s11_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc0447fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46415dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import blankline_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88a2c333",
   "metadata": {},
   "outputs": [],
   "source": [
    " s7 = \"He arrived at 3:00 pm.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "595b65e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He arrived at 3:00 pm.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s7_blank = blankline_tokenize(s7)\n",
    "s7_blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "be3f26ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s7_blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf3e7e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "s8 = \"I bought these items: books, pencils, and pens.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b5123183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I bought these items: books, pencils, and pens.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s8_blank = blankline_tokenize(s8)\n",
    "s8_blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb7e4b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s8_blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e5d8322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fcb2e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s8 = \"I bought these items: books, pencils, and pens.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5c88402a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'bought', 'these', 'items:', 'books,', 'pencils,', 'and', 'pens.']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s8_wt = WhitespaceTokenizer().tokenize(s8)\n",
    "s8_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3aa18343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s8_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5b962eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams,trigrams,ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "af2fa2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 =  \"Well, we couldn't have this predictable, cliche-ridden, \\\"Touched by an Angel\\\" (a show creator John Masius worked on) wanna-be if she didn't.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fc880bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Well',\n",
       " ',',\n",
       " 'we',\n",
       " 'could',\n",
       " \"n't\",\n",
       " 'have',\n",
       " 'this',\n",
       " 'predictable',\n",
       " ',',\n",
       " 'cliche-ridden',\n",
       " ',',\n",
       " '``',\n",
       " 'Touched',\n",
       " 'by',\n",
       " 'an',\n",
       " 'Angel',\n",
       " \"''\",\n",
       " '(',\n",
       " 'a',\n",
       " 'show',\n",
       " 'creator',\n",
       " 'John',\n",
       " 'Masius',\n",
       " 'worked',\n",
       " 'on',\n",
       " ')',\n",
       " 'wanna-be',\n",
       " 'if',\n",
       " 'she',\n",
       " 'did',\n",
       " \"n't\",\n",
       " '.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_tokens = word_tokenize(s3)\n",
    "s3_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "222c6920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Well', ','),\n",
       " (',', 'we'),\n",
       " ('we', 'could'),\n",
       " ('could', \"n't\"),\n",
       " (\"n't\", 'have'),\n",
       " ('have', 'this'),\n",
       " ('this', 'predictable'),\n",
       " ('predictable', ','),\n",
       " (',', 'cliche-ridden'),\n",
       " ('cliche-ridden', ','),\n",
       " (',', '``'),\n",
       " ('``', 'Touched'),\n",
       " ('Touched', 'by'),\n",
       " ('by', 'an'),\n",
       " ('an', 'Angel'),\n",
       " ('Angel', \"''\"),\n",
       " (\"''\", '('),\n",
       " ('(', 'a'),\n",
       " ('a', 'show'),\n",
       " ('show', 'creator'),\n",
       " ('creator', 'John'),\n",
       " ('John', 'Masius'),\n",
       " ('Masius', 'worked'),\n",
       " ('worked', 'on'),\n",
       " ('on', ')'),\n",
       " (')', 'wanna-be'),\n",
       " ('wanna-be', 'if'),\n",
       " ('if', 'she'),\n",
       " ('she', 'did'),\n",
       " ('did', \"n't\"),\n",
       " (\"n't\", '.')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_bigrams = list(nltk.bigrams(s3_tokens))\n",
    "s3_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "798c1775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Well', ',', 'we'),\n",
       " (',', 'we', 'could'),\n",
       " ('we', 'could', \"n't\"),\n",
       " ('could', \"n't\", 'have'),\n",
       " (\"n't\", 'have', 'this'),\n",
       " ('have', 'this', 'predictable'),\n",
       " ('this', 'predictable', ','),\n",
       " ('predictable', ',', 'cliche-ridden'),\n",
       " (',', 'cliche-ridden', ','),\n",
       " ('cliche-ridden', ',', '``'),\n",
       " (',', '``', 'Touched'),\n",
       " ('``', 'Touched', 'by'),\n",
       " ('Touched', 'by', 'an'),\n",
       " ('by', 'an', 'Angel'),\n",
       " ('an', 'Angel', \"''\"),\n",
       " ('Angel', \"''\", '('),\n",
       " (\"''\", '(', 'a'),\n",
       " ('(', 'a', 'show'),\n",
       " ('a', 'show', 'creator'),\n",
       " ('show', 'creator', 'John'),\n",
       " ('creator', 'John', 'Masius'),\n",
       " ('John', 'Masius', 'worked'),\n",
       " ('Masius', 'worked', 'on'),\n",
       " ('worked', 'on', ')'),\n",
       " ('on', ')', 'wanna-be'),\n",
       " (')', 'wanna-be', 'if'),\n",
       " ('wanna-be', 'if', 'she'),\n",
       " ('if', 'she', 'did'),\n",
       " ('she', 'did', \"n't\"),\n",
       " ('did', \"n't\", '.')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_trigrams = list(nltk.trigrams(s3_tokens))\n",
    "s3_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9ba1fff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Well', ',', 'we', 'could'),\n",
       " (',', 'we', 'could', \"n't\"),\n",
       " ('we', 'could', \"n't\", 'have'),\n",
       " ('could', \"n't\", 'have', 'this'),\n",
       " (\"n't\", 'have', 'this', 'predictable'),\n",
       " ('have', 'this', 'predictable', ','),\n",
       " ('this', 'predictable', ',', 'cliche-ridden'),\n",
       " ('predictable', ',', 'cliche-ridden', ','),\n",
       " (',', 'cliche-ridden', ',', '``'),\n",
       " ('cliche-ridden', ',', '``', 'Touched'),\n",
       " (',', '``', 'Touched', 'by'),\n",
       " ('``', 'Touched', 'by', 'an'),\n",
       " ('Touched', 'by', 'an', 'Angel'),\n",
       " ('by', 'an', 'Angel', \"''\"),\n",
       " ('an', 'Angel', \"''\", '('),\n",
       " ('Angel', \"''\", '(', 'a'),\n",
       " (\"''\", '(', 'a', 'show'),\n",
       " ('(', 'a', 'show', 'creator'),\n",
       " ('a', 'show', 'creator', 'John'),\n",
       " ('show', 'creator', 'John', 'Masius'),\n",
       " ('creator', 'John', 'Masius', 'worked'),\n",
       " ('John', 'Masius', 'worked', 'on'),\n",
       " ('Masius', 'worked', 'on', ')'),\n",
       " ('worked', 'on', ')', 'wanna-be'),\n",
       " ('on', ')', 'wanna-be', 'if'),\n",
       " (')', 'wanna-be', 'if', 'she'),\n",
       " ('wanna-be', 'if', 'she', 'did'),\n",
       " ('if', 'she', 'did', \"n't\"),\n",
       " ('she', 'did', \"n't\", '.')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_ngrams = list(nltk.ngrams(s3_tokens,4))\n",
    "s3_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6f2fae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "89bef525",
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1e567ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'play'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('playing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "599e6bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'affect'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('affection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "71203bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "giving:give\n",
      "gave:gave\n",
      "loving:love\n"
     ]
    }
   ],
   "source": [
    "word_stem = ['give', 'giving','gave','loving']\n",
    "\n",
    "for words in word_stem:\n",
    "    print(words + ':' + pst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b55e2f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "774532af",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2644c429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:giv\n",
      "giving:giv\n",
      "gave:gav\n",
      "loving:lov\n"
     ]
    }
   ],
   "source": [
    "for words in word_stem:\n",
    "    print(words + ':' + lst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1acad5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_len = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "088c940f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['give', 'giving', 'gave', 'loving']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5454909b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "giving:giving\n",
      "gave:gave\n",
      "loving:loving\n"
     ]
    }
   ],
   "source": [
    "for words in word_stem:\n",
    "    print(words + ':' + word_len.lemmatize(words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ed047fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b142fadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "31778eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['অতএব',\n",
       " 'অথচ',\n",
       " 'অথবা',\n",
       " 'অনুযায়ী',\n",
       " 'অনেক',\n",
       " 'অনেকে',\n",
       " 'অনেকেই',\n",
       " 'অন্তত',\n",
       " 'অন্য',\n",
       " 'অবধি',\n",
       " 'অবশ্য',\n",
       " 'অর্থাত',\n",
       " 'আই',\n",
       " 'আগামী',\n",
       " 'আগে',\n",
       " 'আগেই',\n",
       " 'আছে',\n",
       " 'আজ',\n",
       " 'আদ্যভাগে',\n",
       " 'আপনার',\n",
       " 'আপনি',\n",
       " 'আবার',\n",
       " 'আমরা',\n",
       " 'আমাকে',\n",
       " 'আমাদের',\n",
       " 'আমার',\n",
       " 'আমি',\n",
       " 'আর',\n",
       " 'আরও',\n",
       " 'ই',\n",
       " 'ইত্যাদি',\n",
       " 'ইহা',\n",
       " 'উচিত',\n",
       " 'উত্তর',\n",
       " 'উনি',\n",
       " 'উপর',\n",
       " 'উপরে',\n",
       " 'এ',\n",
       " 'এঁদের',\n",
       " 'এঁরা',\n",
       " 'এই',\n",
       " 'একই',\n",
       " 'একটি',\n",
       " 'একবার',\n",
       " 'একে',\n",
       " 'এক্',\n",
       " 'এখন',\n",
       " 'এখনও',\n",
       " 'এখানে',\n",
       " 'এখানেই',\n",
       " 'এটা',\n",
       " 'এটাই',\n",
       " 'এটি',\n",
       " 'এত',\n",
       " 'এতটাই',\n",
       " 'এতে',\n",
       " 'এদের',\n",
       " 'এব',\n",
       " 'এবং',\n",
       " 'এবার',\n",
       " 'এমন',\n",
       " 'এমনকী',\n",
       " 'এমনি',\n",
       " 'এর',\n",
       " 'এরা',\n",
       " 'এল',\n",
       " 'এস',\n",
       " 'এসে',\n",
       " 'ঐ',\n",
       " 'ও',\n",
       " 'ওঁদের',\n",
       " 'ওঁর',\n",
       " 'ওঁরা',\n",
       " 'ওই',\n",
       " 'ওকে',\n",
       " 'ওখানে',\n",
       " 'ওদের',\n",
       " 'ওর',\n",
       " 'ওরা',\n",
       " 'কখনও',\n",
       " 'কত',\n",
       " 'কবে',\n",
       " 'কমনে',\n",
       " 'কয়েক',\n",
       " 'কয়েকটি',\n",
       " 'করছে',\n",
       " 'করছেন',\n",
       " 'করতে',\n",
       " 'করবে',\n",
       " 'করবেন',\n",
       " 'করলে',\n",
       " 'করলেন',\n",
       " 'করা',\n",
       " 'করাই',\n",
       " 'করায়',\n",
       " 'করার',\n",
       " 'করি',\n",
       " 'করিতে',\n",
       " 'করিয়া',\n",
       " 'করিয়ে',\n",
       " 'করে',\n",
       " 'করেই',\n",
       " 'করেছিলেন',\n",
       " 'করেছে',\n",
       " 'করেছেন',\n",
       " 'করেন',\n",
       " 'কাউকে',\n",
       " 'কাছ',\n",
       " 'কাছে',\n",
       " 'কাজ',\n",
       " 'কাজে',\n",
       " 'কারও',\n",
       " 'কারণ',\n",
       " 'কি',\n",
       " 'কিংবা',\n",
       " 'কিছু',\n",
       " 'কিছুই',\n",
       " 'কিন্তু',\n",
       " 'কী',\n",
       " 'কে',\n",
       " 'কেউ',\n",
       " 'কেউই',\n",
       " 'কেখা',\n",
       " 'কেন',\n",
       " 'কোটি',\n",
       " 'কোন',\n",
       " 'কোনও',\n",
       " 'কোনো',\n",
       " 'ক্ষেত্রে',\n",
       " 'কয়েক',\n",
       " 'খুব',\n",
       " 'গিয়ে',\n",
       " 'গিয়েছে',\n",
       " 'গিয়ে',\n",
       " 'গুলি',\n",
       " 'গেছে',\n",
       " 'গেল',\n",
       " 'গেলে',\n",
       " 'গোটা',\n",
       " 'চলে',\n",
       " 'চান',\n",
       " 'চায়',\n",
       " 'চার',\n",
       " 'চালু',\n",
       " 'চেয়ে',\n",
       " 'চেষ্টা',\n",
       " 'ছাড়া',\n",
       " 'ছাড়াও',\n",
       " 'ছিল',\n",
       " 'ছিলেন',\n",
       " 'জন',\n",
       " 'জনকে',\n",
       " 'জনের',\n",
       " 'জন্য',\n",
       " 'জন্যওজে',\n",
       " 'জানতে',\n",
       " 'জানা',\n",
       " 'জানানো',\n",
       " 'জানায়',\n",
       " 'জানিয়ে',\n",
       " 'জানিয়েছে',\n",
       " 'জে',\n",
       " 'জ্নজন',\n",
       " 'টি',\n",
       " 'ঠিক',\n",
       " 'তখন',\n",
       " 'তত',\n",
       " 'তথা',\n",
       " 'তবু',\n",
       " 'তবে',\n",
       " 'তা',\n",
       " 'তাঁকে',\n",
       " 'তাঁদের',\n",
       " 'তাঁর',\n",
       " 'তাঁরা',\n",
       " 'তাঁাহারা',\n",
       " 'তাই',\n",
       " 'তাও',\n",
       " 'তাকে',\n",
       " 'তাতে',\n",
       " 'তাদের',\n",
       " 'তার',\n",
       " 'তারপর',\n",
       " 'তারা',\n",
       " 'তারৈ',\n",
       " 'তাহলে',\n",
       " 'তাহা',\n",
       " 'তাহাতে',\n",
       " 'তাহার',\n",
       " 'তিনঐ',\n",
       " 'তিনি',\n",
       " 'তিনিও',\n",
       " 'তুমি',\n",
       " 'তুলে',\n",
       " 'তেমন',\n",
       " 'তো',\n",
       " 'তোমার',\n",
       " 'থাকবে',\n",
       " 'থাকবেন',\n",
       " 'থাকা',\n",
       " 'থাকায়',\n",
       " 'থাকে',\n",
       " 'থাকেন',\n",
       " 'থেকে',\n",
       " 'থেকেই',\n",
       " 'থেকেও',\n",
       " 'দিকে',\n",
       " 'দিতে',\n",
       " 'দিন',\n",
       " 'দিয়ে',\n",
       " 'দিয়েছে',\n",
       " 'দিয়েছেন',\n",
       " 'দিলেন',\n",
       " 'দু',\n",
       " 'দুই',\n",
       " 'দুটি',\n",
       " 'দুটো',\n",
       " 'দেওয়া',\n",
       " 'দেওয়ার',\n",
       " 'দেওয়া',\n",
       " 'দেখতে',\n",
       " 'দেখা',\n",
       " 'দেখে',\n",
       " 'দেন',\n",
       " 'দেয়',\n",
       " 'দ্বারা',\n",
       " 'ধরা',\n",
       " 'ধরে',\n",
       " 'ধামার',\n",
       " 'নতুন',\n",
       " 'নয়',\n",
       " 'না',\n",
       " 'নাই',\n",
       " 'নাকি',\n",
       " 'নাগাদ',\n",
       " 'নানা',\n",
       " 'নিজে',\n",
       " 'নিজেই',\n",
       " 'নিজেদের',\n",
       " 'নিজের',\n",
       " 'নিতে',\n",
       " 'নিয়ে',\n",
       " 'নিয়ে',\n",
       " 'নেই',\n",
       " 'নেওয়া',\n",
       " 'নেওয়ার',\n",
       " 'নেওয়া',\n",
       " 'নয়',\n",
       " 'পক্ষে',\n",
       " 'পর',\n",
       " 'পরে',\n",
       " 'পরেই',\n",
       " 'পরেও',\n",
       " 'পর্যন্ত',\n",
       " 'পাওয়া',\n",
       " 'পাচ',\n",
       " 'পারি',\n",
       " 'পারে',\n",
       " 'পারেন',\n",
       " 'পি',\n",
       " 'পেয়ে',\n",
       " 'পেয়্র্',\n",
       " 'প্রতি',\n",
       " 'প্রথম',\n",
       " 'প্রভৃতি',\n",
       " 'প্রযন্ত',\n",
       " 'প্রাথমিক',\n",
       " 'প্রায়',\n",
       " 'প্রায়',\n",
       " 'ফলে',\n",
       " 'ফিরে',\n",
       " 'ফের',\n",
       " 'বক্তব্য',\n",
       " 'বদলে',\n",
       " 'বন',\n",
       " 'বরং',\n",
       " 'বলতে',\n",
       " 'বলল',\n",
       " 'বললেন',\n",
       " 'বলা',\n",
       " 'বলে',\n",
       " 'বলেছেন',\n",
       " 'বলেন',\n",
       " 'বসে',\n",
       " 'বহু',\n",
       " 'বা',\n",
       " 'বাদে',\n",
       " 'বার',\n",
       " 'বি',\n",
       " 'বিনা',\n",
       " 'বিভিন্ন',\n",
       " 'বিশেষ',\n",
       " 'বিষয়টি',\n",
       " 'বেশ',\n",
       " 'বেশি',\n",
       " 'ব্যবহার',\n",
       " 'ব্যাপারে',\n",
       " 'ভাবে',\n",
       " 'ভাবেই',\n",
       " 'মতো',\n",
       " 'মতোই',\n",
       " 'মধ্যভাগে',\n",
       " 'মধ্যে',\n",
       " 'মধ্যেই',\n",
       " 'মধ্যেও',\n",
       " 'মনে',\n",
       " 'মাত্র',\n",
       " 'মাধ্যমে',\n",
       " 'মোট',\n",
       " 'মোটেই',\n",
       " 'যখন',\n",
       " 'যত',\n",
       " 'যতটা',\n",
       " 'যথেষ্ট',\n",
       " 'যদি',\n",
       " 'যদিও',\n",
       " 'যা',\n",
       " 'যাঁর',\n",
       " 'যাঁরা',\n",
       " 'যাওয়া',\n",
       " 'যাওয়ার',\n",
       " 'যাওয়া',\n",
       " 'যাকে',\n",
       " 'যাচ্ছে',\n",
       " 'যাতে',\n",
       " 'যাদের',\n",
       " 'যান',\n",
       " 'যাবে',\n",
       " 'যায়',\n",
       " 'যার',\n",
       " 'যারা',\n",
       " 'যিনি',\n",
       " 'যে',\n",
       " 'যেখানে',\n",
       " 'যেতে',\n",
       " 'যেন',\n",
       " 'যেমন',\n",
       " 'র',\n",
       " 'রকম',\n",
       " 'রয়েছে',\n",
       " 'রাখা',\n",
       " 'রেখে',\n",
       " 'লক্ষ',\n",
       " 'শুধু',\n",
       " 'শুরু',\n",
       " 'সঙ্গে',\n",
       " 'সঙ্গেও',\n",
       " 'সব',\n",
       " 'সবার',\n",
       " 'সমস্ত',\n",
       " 'সম্প্রতি',\n",
       " 'সহ',\n",
       " 'সহিত',\n",
       " 'সাধারণ',\n",
       " 'সামনে',\n",
       " 'সি',\n",
       " 'সুতরাং',\n",
       " 'সে',\n",
       " 'সেই',\n",
       " 'সেখান',\n",
       " 'সেখানে',\n",
       " 'সেটা',\n",
       " 'সেটাই',\n",
       " 'সেটাও',\n",
       " 'সেটি',\n",
       " 'স্পষ্ট',\n",
       " 'স্বয়ং',\n",
       " 'হইতে',\n",
       " 'হইবে',\n",
       " 'হইয়া',\n",
       " 'হওয়া',\n",
       " 'হওয়ায়',\n",
       " 'হওয়ার',\n",
       " 'হচ্ছে',\n",
       " 'হত',\n",
       " 'হতে',\n",
       " 'হতেই',\n",
       " 'হন',\n",
       " 'হবে',\n",
       " 'হবেন',\n",
       " 'হয়',\n",
       " 'হয়তো',\n",
       " 'হয়নি',\n",
       " 'হয়ে',\n",
       " 'হয়েই',\n",
       " 'হয়েছিল',\n",
       " 'হয়েছে',\n",
       " 'হয়েছেন',\n",
       " 'হল',\n",
       " 'হলে',\n",
       " 'হলেই',\n",
       " 'হলেও',\n",
       " 'হলো',\n",
       " 'হাজার',\n",
       " 'হিসাবে',\n",
       " 'হৈলে',\n",
       " 'হোক',\n",
       " 'হয়']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('bengali')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "00b65228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aber',\n",
       " 'alle',\n",
       " 'allem',\n",
       " 'allen',\n",
       " 'aller',\n",
       " 'alles',\n",
       " 'als',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ander',\n",
       " 'andere',\n",
       " 'anderem',\n",
       " 'anderen',\n",
       " 'anderer',\n",
       " 'anderes',\n",
       " 'anderm',\n",
       " 'andern',\n",
       " 'anderr',\n",
       " 'anders',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aus',\n",
       " 'bei',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bist',\n",
       " 'da',\n",
       " 'damit',\n",
       " 'dann',\n",
       " 'der',\n",
       " 'den',\n",
       " 'des',\n",
       " 'dem',\n",
       " 'die',\n",
       " 'das',\n",
       " 'dass',\n",
       " 'daß',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'denselben',\n",
       " 'desselben',\n",
       " 'demselben',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'dasselbe',\n",
       " 'dazu',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deinen',\n",
       " 'deiner',\n",
       " 'deines',\n",
       " 'denn',\n",
       " 'derer',\n",
       " 'dessen',\n",
       " 'dich',\n",
       " 'dir',\n",
       " 'du',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'doch',\n",
       " 'dort',\n",
       " 'durch',\n",
       " 'ein',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'eines',\n",
       " 'einig',\n",
       " 'einige',\n",
       " 'einigem',\n",
       " 'einigen',\n",
       " 'einiger',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'er',\n",
       " 'ihn',\n",
       " 'ihm',\n",
       " 'es',\n",
       " 'etwas',\n",
       " 'euer',\n",
       " 'eure',\n",
       " 'eurem',\n",
       " 'euren',\n",
       " 'eurer',\n",
       " 'eures',\n",
       " 'für',\n",
       " 'gegen',\n",
       " 'gewesen',\n",
       " 'hab',\n",
       " 'habe',\n",
       " 'haben',\n",
       " 'hat',\n",
       " 'hatte',\n",
       " 'hatten',\n",
       " 'hier',\n",
       " 'hin',\n",
       " 'hinter',\n",
       " 'ich',\n",
       " 'mich',\n",
       " 'mir',\n",
       " 'ihr',\n",
       " 'ihre',\n",
       " 'ihrem',\n",
       " 'ihren',\n",
       " 'ihrer',\n",
       " 'ihres',\n",
       " 'euch',\n",
       " 'im',\n",
       " 'in',\n",
       " 'indem',\n",
       " 'ins',\n",
       " 'ist',\n",
       " 'jede',\n",
       " 'jedem',\n",
       " 'jeden',\n",
       " 'jeder',\n",
       " 'jedes',\n",
       " 'jene',\n",
       " 'jenem',\n",
       " 'jenen',\n",
       " 'jener',\n",
       " 'jenes',\n",
       " 'jetzt',\n",
       " 'kann',\n",
       " 'kein',\n",
       " 'keine',\n",
       " 'keinem',\n",
       " 'keinen',\n",
       " 'keiner',\n",
       " 'keines',\n",
       " 'können',\n",
       " 'könnte',\n",
       " 'machen',\n",
       " 'man',\n",
       " 'manche',\n",
       " 'manchem',\n",
       " 'manchen',\n",
       " 'mancher',\n",
       " 'manches',\n",
       " 'mein',\n",
       " 'meine',\n",
       " 'meinem',\n",
       " 'meinen',\n",
       " 'meiner',\n",
       " 'meines',\n",
       " 'mit',\n",
       " 'muss',\n",
       " 'musste',\n",
       " 'nach',\n",
       " 'nicht',\n",
       " 'nichts',\n",
       " 'noch',\n",
       " 'nun',\n",
       " 'nur',\n",
       " 'ob',\n",
       " 'oder',\n",
       " 'ohne',\n",
       " 'sehr',\n",
       " 'sein',\n",
       " 'seine',\n",
       " 'seinem',\n",
       " 'seinen',\n",
       " 'seiner',\n",
       " 'seines',\n",
       " 'selbst',\n",
       " 'sich',\n",
       " 'sie',\n",
       " 'ihnen',\n",
       " 'sind',\n",
       " 'so',\n",
       " 'solche',\n",
       " 'solchem',\n",
       " 'solchen',\n",
       " 'solcher',\n",
       " 'solches',\n",
       " 'soll',\n",
       " 'sollte',\n",
       " 'sondern',\n",
       " 'sonst',\n",
       " 'über',\n",
       " 'um',\n",
       " 'und',\n",
       " 'uns',\n",
       " 'unsere',\n",
       " 'unserem',\n",
       " 'unseren',\n",
       " 'unser',\n",
       " 'unseres',\n",
       " 'unter',\n",
       " 'viel',\n",
       " 'vom',\n",
       " 'von',\n",
       " 'vor',\n",
       " 'während',\n",
       " 'war',\n",
       " 'waren',\n",
       " 'warst',\n",
       " 'was',\n",
       " 'weg',\n",
       " 'weil',\n",
       " 'weiter',\n",
       " 'welche',\n",
       " 'welchem',\n",
       " 'welchen',\n",
       " 'welcher',\n",
       " 'welches',\n",
       " 'wenn',\n",
       " 'werde',\n",
       " 'werden',\n",
       " 'wie',\n",
       " 'wieder',\n",
       " 'will',\n",
       " 'wir',\n",
       " 'wird',\n",
       " 'wirst',\n",
       " 'wo',\n",
       " 'wollen',\n",
       " 'wollte',\n",
       " 'würde',\n",
       " 'würden',\n",
       " 'zu',\n",
       " 'zum',\n",
       " 'zur',\n",
       " 'zwar',\n",
       " 'zwischen']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "214d5ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2ccc715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TreebankWordTokenizer is an NLTK tokenizer designed to mimic how words are tokenized in the Penn Treebank corpus — a famous linguistic dataset used in NLP research.\n",
    "#It doesn’t just split text by spaces.\n",
    "# It applies linguistic rules to handle punctuation, contractions, quotes, etc., correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b4071009",
   "metadata": {},
   "outputs": [],
   "source": [
    "sx1 = '\\xabNow that I can do.\\xbb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9b1db111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b9b65e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "69699801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['«Now', 'that', 'I', 'can', 'do.»']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sx1_tree = tokenizer.tokenize(sx1)\n",
    "sx1_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e7b81de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sx2 = \"He's going home because he can't stay.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "20451ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"He's going home because he can't stay.\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "89470d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', \"'s\", 'going', 'home', 'because', 'he', 'ca', \"n't\", 'stay', '.']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sx2_tree = tokenizer.tokenize(sx2)\n",
    "sx2_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cbac7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "sx3 = \"John's book is on Mary's table.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0d9d5101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', \"'s\", 'book', 'is', 'on', 'Mary', \"'s\", 'table', '.']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sx3_tree = tokenizer.tokenize(sx3)\n",
    "sx3_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e36747fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0d974f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_sent = \"I called Dr. Jones. I called Dr. Jones.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e652453c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'called', 'Dr.', 'Jones', '.', 'I', 'called', 'Dr.', 'Jones', '.']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NE_tokens = word_tokenize(NE_sent)\n",
    "NE_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "db22880d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('called', 'VBD'),\n",
       " ('Dr.', 'NNP'),\n",
       " ('Jones', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('called', 'VBD'),\n",
       " ('Dr.', 'NNP'),\n",
       " ('Jones', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NE_tags = nltk.pos_tag(NE_tokens)\n",
    "NE_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5755f84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NE_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bc8cc6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names ENtitity Recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "33fe5ec7",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mmaxent_ne_chunker_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('maxent_ne_chunker_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mchunkers/maxent_ne_chunker_tab/english_ace_multiclass/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\LENOVO/nltk_data'\n    - 'C:\\\\Users\\\\LENOVO\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\LENOVO\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\LENOVO\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\LENOVO\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m NE_NER \u001b[38;5;241m=\u001b[39m \u001b[43mne_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNE_tags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(NE_NER)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\chunk\\__init__.py:192\u001b[0m, in \u001b[0;36mne_chunk\u001b[1;34m(tagged_tokens, binary)\u001b[0m\n\u001b[0;32m    190\u001b[0m     chunker \u001b[38;5;241m=\u001b[39m ne_chunker(fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     chunker \u001b[38;5;241m=\u001b[39m \u001b[43mne_chunker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mparse(tagged_tokens)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\chunk\\__init__.py:174\u001b[0m, in \u001b[0;36mne_chunker\u001b[1;34m(fmt)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mne_chunker\u001b[39m(fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03m    Load NLTK's currently recommended named entity chunker.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMaxent_NE_Chunker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\chunk\\named_entity.py:329\u001b[0m, in \u001b[0;36mMaxent_NE_Chunker.__init__\u001b[1;34m(self, fmt)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fmt \u001b[38;5;241m=\u001b[39m fmt\n\u001b[1;32m--> 329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tab_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunkers/maxent_ne_chunker_tab/english_ace_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfmt\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_params()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mmaxent_ne_chunker_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('maxent_ne_chunker_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mchunkers/maxent_ne_chunker_tab/english_ace_multiclass/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\LENOVO/nltk_data'\n    - 'C:\\\\Users\\\\LENOVO\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\LENOVO\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\LENOVO\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\LENOVO\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "NE_NER = ne_chunk(NE_tags)\n",
    "print(NE_NER) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1fc5b268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "32e3fa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6fae41c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "33fbacc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  called/VBD\n",
      "  Dr./NNP\n",
      "  (PERSON Jones/NNP)\n",
      "  ./.\n",
      "  I/PRP\n",
      "  called/VBD\n",
      "  Dr./NNP\n",
      "  (PERSON Jones/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "NE_NER = ne_chunk(NE_tags)\n",
    "print(NE_NER) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e1c9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
